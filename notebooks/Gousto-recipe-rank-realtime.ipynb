{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gousto Recipe Rank Forecasting: MSc Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.getcwd()\n",
    "file_path = os.path.join(directory, '..', 'data', 'extended_training_df_619.json')\n",
    "\n",
    "# Read JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "fields = [field['name'] for field in data['schema']['fields'] if field['name'] != 'index']\n",
    "records = data.get('data', [])\n",
    "\n",
    "# Convert the records to a DataFrame\n",
    "df = pd.DataFrame(records, columns=fields)\n",
    "df['item_id'] = pd.to_numeric(df['item_id'], errors='coerce')\n",
    "df.drop(columns=['contamination_outlier', 'uptake_at_lead_day_-6'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['recipe_rank'] = df.groupby('menu_week')['recipe_uptake'].rank(method='first', ascending=False) - 1\n",
    "df['recipe_rank'] = df['recipe_rank'].astype(int)\n",
    "\n",
    "columns_to_rank = [f'uptake_at_lead_day_{j}' for j in range(-5, 19)]\n",
    "for i, col in enumerate(columns_to_rank):\n",
    "    df[col] = df.groupby('menu_week')[col].rank(method='first', pct=True, ascending=True)\n",
    "    if i > 0:\n",
    "        prev_col = columns_to_rank[i-1]  # Previous column\n",
    "        df[f'diff_uptake_at_lead_day_{i-6}'] = df[prev_col] - df[col]\n",
    "\n",
    "df_live = df.copy()\n",
    "df_live = df_live[df_live['menu_week'] >= 488]\n",
    "\n",
    "columns_to_remove = df.filter(like=\"uptake_at_lead_day\").columns\n",
    "df.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "# Drop helper columns\n",
    "df = df.drop(columns=['recipe_uptake'])\n",
    "df_live = df_live.drop(columns=['recipe_uptake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank recipe_rank within each menu_week\n",
    "df_live_pct = df_live.copy()\n",
    "df_live_pct['recipe_rank'] = df_live_pct.groupby('menu_week')['recipe_rank'].rank(method='first', pct=True, ascending=False)\n",
    "\n",
    "# Drop uptake_at_lead_day and recipe_rank columns, and compute correlation with the target\n",
    "non_uptake_features = df_live_pct.drop(columns=df_live_pct.filter(regex='uptake_at_lead_day|embeddings').columns)\n",
    "non_uptake_features = non_uptake_features.drop(columns=['recipe_rank'])\n",
    "non_uptake_corr = pd.concat([non_uptake_features, df_live_pct['recipe_rank']], axis=1).corr()\n",
    "\n",
    "# Sort features by absolute correlation and select top 20\n",
    "sorted_corr = non_uptake_corr[['recipe_rank']].dropna().reindex(non_uptake_corr[['recipe_rank']].dropna().apply(lambda x: abs(x)).sort_values(by='recipe_rank', ascending=False).index)\n",
    "\n",
    "# Plot the top 20 most correlated features\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(sorted_corr, annot=True, fmt='.3f', cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(f'Correlations between non uptake and embedding features and recipe rank percentile')\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation for uptake_at_lead_day features\n",
    "uptake_features = df_live.filter(regex=r'^uptake_at_lead_day')\n",
    "uptake_corr = pd.concat([uptake_features, df_live_pct['recipe_rank']], axis=1).corr()\n",
    "\n",
    "# Plot correlation for uptake_at_lead_day features\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(uptake_corr[['recipe_rank']].dropna(), annot=True, fmt='.8f', cmap='coolwarm', vmin=0.90, vmax=1)\n",
    "plt.title('Correlation between \"uptake_at_lead_day\" features and recipe rank percentile')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Data Normalisation and Custom Time Series Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalise data\n",
    "def normalize_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert back to DataFrame to maintain the index\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "# Function for custom time series cross validation split, for the final 3 menu weeks\n",
    "def custom_time_series_cv(X_train, n_validation_weeks=3):\n",
    "    validation_weeks = X_train['menu_week'].unique()[-n_validation_weeks:]\n",
    "    for valid_week in validation_weeks:\n",
    "        train_df = X_train[X_train['menu_week'] < valid_week]\n",
    "        valid_df = X_train[X_train['menu_week'] == valid_week]\n",
    "        yield train_df.index, valid_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate predictions and plot error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions_live(all_errors, feature_errors, method, days_before_target=18, total_week_iterations=20):\n",
    "    average_errors = []\n",
    "    error_sems = []  # To store 2 * standard errors for model predictions (95% CI)\n",
    "    feature_maes = []  # To store average feature MAE\n",
    "    feature_mae_sems = []  # To store 2 * standard errors for feature MAE (95% CI)\n",
    "\n",
    "    for i in range(days_before_target + 7):\n",
    "        errors_for_week = [iteration[i]['mean_absolute_error'] for iteration in all_errors if i < len(iteration)]\n",
    "        average_error = np.mean(errors_for_week)\n",
    "        error_sem = (np.std(errors_for_week) / np.sqrt(len(errors_for_week))) * 2  # 2 * standard error for 95% CI\n",
    "        average_errors.append({'days_before': days_before_target - i, 'average_mean_absolute_error': average_error})\n",
    "        error_sems.append({'days_before': days_before_target - i, 'error_sem': error_sem})\n",
    "\n",
    "        # Process feature MAE\n",
    "        feature_maes_for_week = [iteration[i]['feature_mae'] for iteration in feature_errors if i < len(iteration)]\n",
    "        if feature_maes_for_week:\n",
    "            feature_avg_mae = np.mean(feature_maes_for_week)\n",
    "            feature_sem = (np.std(feature_maes_for_week) / np.sqrt(len(feature_maes_for_week))) * 2  # 2 * standard error for 95% CI\n",
    "            feature_maes.append({'days_before': days_before_target - i, 'feature_mean_absolute_error': feature_avg_mae})\n",
    "            feature_mae_sems.append({'days_before': days_before_target - i, 'feature_sem': feature_sem})\n",
    "\n",
    "    average_errors_df = pd.DataFrame(average_errors)\n",
    "    error_sems_df = pd.DataFrame(error_sems)\n",
    "    feature_maes_df = pd.DataFrame(feature_maes)\n",
    "    feature_mae_sems_df = pd.DataFrame(feature_mae_sems)\n",
    "\n",
    "    # Create the plot with specified colors and enhancements\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot for feature uptake_at_lead_day MAE with 95% CI error bars (2 * standard errors)\n",
    "    plt.errorbar(feature_maes_df['days_before'], feature_maes_df['feature_mean_absolute_error'], \n",
    "                 yerr=feature_mae_sems_df['feature_sem'], fmt='--s', capsize=5, \n",
    "                 color='orange', ecolor='red', elinewidth=2, markerfacecolor='orange', label=f'Uptake Feature MAE (95% CI)')\n",
    "    \n",
    "    # Plot for model predictions with 95% CI error bars (2 * standard errors)\n",
    "    plt.errorbar(average_errors_df['days_before'], average_errors_df['average_mean_absolute_error'], \n",
    "                 yerr=error_sems_df['error_sem'], fmt='-o', capsize=5, \n",
    "                 color='purple', ecolor='blue', elinewidth=2, markerfacecolor='purple', label='Predicted MAE (95% CI)')\n",
    "    \n",
    "    plt.ylim(-0.005, 0.165)\n",
    "    plt.yticks(np.arange(0, 0.17, 0.01))\n",
    "    plt.xticks(np.arange(days_before_target, -7, -1))\n",
    "    plt.xlim(-7, days_before_target+1)\n",
    "    plt.xlabel('Days Before Target')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.title(f'{method}: Average Mean Absolute Error over {days_before_target} Days Before Target Across {total_week_iterations} Iterations')\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.grid(True, which='minor', axis='y', linestyle='--', linewidth=0.4)\n",
    "    plt.gca().yaxis.set_major_formatter(ScalarFormatter(useOffset=False))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_regression_predictions(X_train, y_train, X_test):\n",
    "    X_train, X_test = normalize_data(X_train, X_test)\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    predictions = lr_model.predict(X_test)\n",
    "    return pd.Series(predictions, index=X_test.index)\n",
    "\n",
    "#XGBoost\n",
    "def get_xgboost_predictions(X_train, y_train, X_test):\n",
    "    params = {\n",
    "        'objective': 'reg:logistic',\n",
    "        'eval_metric': 'mae',\n",
    "        'max_depth': 6,\n",
    "    }\n",
    "    model_xgboost = xgb.XGBRegressor(**params)\n",
    "    model_xgboost.fit(X_train, y_train)\n",
    "    predictions = model_xgboost.predict(X_test)\n",
    "    return pd.Series(predictions, index=X_test.index)\n",
    "\n",
    "#LightGBM\n",
    "def get_lightgbm_predictions(X_train, y_train, X_test):\n",
    "    params = {\n",
    "        'verbose': -1,\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',\n",
    "        'max_depth': 6,\n",
    "    }\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    return pd.Series(predictions, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_forest_predictions(X_train, y_train, X_test):\n",
    "    params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 6,\n",
    "        'criterion': 'friedman_mse',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    return pd.Series(predictions, index=X_test.index)\n",
    "\n",
    "# Lambdarank\n",
    "def get_lgbm_ranker_predictions(X_train, y_train, X_test):\n",
    "    group_train = X_train.groupby('menu_week').size().to_list() \n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.05,\n",
    "        'importance_type': 'split',\n",
    "        'label_gain': list(range(max(group_train))),\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "    model = lgb.LGBMRanker(**params)\n",
    "    model.fit(X_train, y_train, group=group_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    return pd.Series(predictions, index=X_test.index)\n",
    "\n",
    "def get_xgboost_ranker_predictions(X_train, y_train, X_test):\n",
    "    group_train = X_train.groupby('menu_week').size().to_list() \n",
    "    params = {\n",
    "        'objective': 'rank:ndcg',\n",
    "        'eval_metric': 'ndcg',\n",
    "        'learning_rate': 0.15,\n",
    "        'max_depth': 6,\n",
    "        'min_child_weight': 17,\n",
    "        'gamma': 1.0,\n",
    "        'lambda': 0.01,\n",
    "        'alpha': 0.2,\n",
    "        'n_jobs': -1,\n",
    "        'ndcg_exp_gain': False,  # Disable exponential gain for NDCG\n",
    "        'random_state': 42,\n",
    "    }\n",
    "\n",
    "    num_boost_round = 100  # This replaces the 'n_estimators' parameter\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtrain.set_group(group_train)\n",
    "\n",
    "    model = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    predictions = model.predict(dtest)\n",
    "    return pd.Series(predictions, index=X_test.index)\n",
    "\n",
    "def get_catboost_ranker_predictions(X_train, y_train, X_test):\n",
    "    group_train = X_train['menu_week']\n",
    "    params = {\n",
    "        'learning_rate': 0.3,\n",
    "        'iterations': 50,\n",
    "        'loss_function': 'PairLogit',\n",
    "        'verbose': False,\n",
    "        'random_seed': 42,\n",
    "        'bootstrap_type': 'Bernoulli',\n",
    "        'eval_metric': 'NDCG',\n",
    "        'random_seed': 42,\n",
    "    }\n",
    "    model = cb.CatBoostRanker(**params)\n",
    "    model.fit(X_train, y_train, group_id=group_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    return pd.Series(predictions, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run evaluation given algorithm\n",
    "def run_evaluation_live(df, target_week_range, days_before_target=18, method='linear_regression'):\n",
    "    df = df.copy()\n",
    "\n",
    "    total_week_iterations = len(target_week_range)\n",
    "    all_errors = []\n",
    "    feature_errors = []\n",
    "    num_models = 0\n",
    "    total_models = total_week_iterations * (days_before_target + 7)\n",
    "\n",
    "    method_dict = {\n",
    "    'linear_regression': get_linear_regression_predictions,\n",
    "    'xgboost': get_xgboost_predictions,\n",
    "    'lightgbm': get_lightgbm_predictions,\n",
    "    'random_forest': get_random_forest_predictions,\n",
    "    'lgbm_ranker': get_lgbm_ranker_predictions,\n",
    "    'xgboost_ranker': get_xgboost_ranker_predictions,\n",
    "    'catboost_ranker': get_catboost_ranker_predictions,\n",
    "    }\n",
    "\n",
    "    \n",
    "    if method not in method_dict:\n",
    "        raise ValueError(\"Invalid method specified.\")\n",
    "\n",
    "    if (method != 'lgbm_ranker'):\n",
    "        df['recipe_rank'] = df.groupby('menu_week')['recipe_rank'].rank(method='first', pct=True, ascending=False)\n",
    "    \n",
    "\n",
    "    for target_week in target_week_range:\n",
    "        iteration_errors = []\n",
    "        feature_iteration_errors = []\n",
    "        elapsed_times = []\n",
    "        for days_before in range(0, days_before_target + 7):\n",
    "            start_time = time.time()\n",
    "            num_models += 1\n",
    "            # Add the uptake_at_lead_day column for the current days_before\n",
    "            columns_to_keep = [f'uptake_at_lead_day_{days_before_target + 1 - days_before}']\n",
    "            for i in range(0, days_before + 1):\n",
    "                columns_to_keep.append(f'diff_uptake_at_lead_day_{days_before_target + 1 - i}')\n",
    "                \n",
    "            # columns_to_keep = [f'uptake_at_lead_day_{days_before_target + 1 - days_before}', f'diff_uptake_at_lead_day_{days_before_target + 1 - days_before}']\n",
    "            columns_to_remove = [col for col in df.columns if 'uptake_at_lead_day_' in col and col not in columns_to_keep]\n",
    "            df_reduced = df.drop(columns=columns_to_remove)\n",
    "            # columns = df_reduced.filter(like=\"uptake_at_lead_day\").columns\n",
    "            buffer = (max(0, days_before_target + 1 - days_before) // 7) + 1\n",
    "            train_df = df_reduced[df_reduced['menu_week'] <= target_week - buffer]\n",
    "            test_df = df_reduced[df_reduced['menu_week'] == target_week]\n",
    "\n",
    "            X_train = train_df.drop(columns=['recipe_rank'])\n",
    "            y_train = train_df['recipe_rank']\n",
    "\n",
    "            X_test = test_df.drop(columns=['recipe_rank'])\n",
    "            y_test = test_df['recipe_rank']\n",
    "\n",
    "            y_pred_values = method_dict[method](X_train, y_train, X_test)\n",
    "            y_pred_values = pd.Series(y_pred_values).rank(ascending=True, pct=True, method='first').to_numpy()\n",
    "            if (method == 'lgbm_ranker'):\n",
    "                y_test = pd.Series(y_test).rank(ascending=False, pct=True, method='first')\n",
    "                y_pred_values = 1 - y_pred_values\n",
    "            mean_absolute_error_value = mean_absolute_error(y_test.values, y_pred_values)\n",
    "            iteration_errors.append({'days_before': days_before_target - days_before, 'mean_absolute_error': mean_absolute_error_value})\n",
    "\n",
    "            uptake_feature_col = f'uptake_at_lead_day_{days_before_target + 1 - days_before}'\n",
    "            if uptake_feature_col in X_test.columns:\n",
    "                y_feature_test = X_test[uptake_feature_col].values\n",
    "                feature_mae_value = mean_absolute_error(y_test, y_feature_test)\n",
    "                feature_iteration_errors.append({'days_before': days_before_target - days_before, 'feature_mae': feature_mae_value})\n",
    "            else:\n",
    "                feature_iteration_errors.append({'days_before': days_before_target - days_before, 'feature_mae': mean_absolute_error_value})\n",
    "            time_elapsed = time.time() - start_time\n",
    "            elapsed_times.append(time_elapsed)\n",
    "            if len(elapsed_times) > 20:\n",
    "                elapsed_times.pop(0)\n",
    "\n",
    "            if elapsed_times:\n",
    "                avg_time_elapsed = sum(elapsed_times) / len(elapsed_times)\n",
    "                minutes, seconds = divmod(avg_time_elapsed * (total_models - num_models), 60)\n",
    "\n",
    "            print(f\"No. of models progress: {num_models}/{total_models} | Estimated Time Left: {int(minutes)} minutes and {int(seconds)} seconds              \", end='\\r')\n",
    "\n",
    "        all_errors.append(iteration_errors)\n",
    "        feature_errors.append(feature_iteration_errors)\n",
    "        \n",
    "    evaluate_predictions_live(all_errors, feature_errors, method, days_before_target, total_week_iterations)\n",
    "    return all_errors, feature_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setback = 0\n",
    "original_max_week = df['menu_week'].max() + 1 - setback\n",
    "\n",
    "# Run the evaluation\n",
    "total_week_iterations = 52\n",
    "days_before_target = 18\n",
    "target_week_range = range(original_max_week - total_week_iterations, original_max_week)\n",
    "\n",
    "# print(\"Linear Regression:\"); run_evaluation_live(df_live, target_week_range, days_before_target, method='linear_regression')\n",
    "\n",
    "# print(\"LightGBM:\"); run_evaluation_live(df_live, target_week_range, days_before_target, method='lightgbm')\n",
    "\n",
    "# print(\"XGBoost:\"); run_evaluation_live(df_live, target_week_range, days_before_target, method='xgboost')\n",
    "\n",
    "# print(\"Random Forest:\"); run_evaluation_live(df_live, target_week_range, days_before_target, method='random_forest')\n",
    "\n",
    "# print(\"LightGBM Ranker:\") ; run_evaluation_live(df_live, target_week_range, days_before_target, method='lgbm_ranker')\n",
    "\n",
    "# print(\"XGBoost Ranker:\") ; run_evaluation_live(df_live, target_week_range, days_before_target, method='xgboost_ranker')\n",
    "\n",
    "# print(\"CatBoost Ranker:\") ; run_evaluation_live(df_live, target_week_range, days_before_target, method='catboost_ranker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the json file\n",
    "with open('short_term_results.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define all model names\n",
    "model_names = [\n",
    "    \"Real-Time Prediction\", \n",
    "    \"Linear Regression\", \n",
    "    \"LightGBM\", \n",
    "    \"XGBoost\", \n",
    "    \"Random Forest\", \n",
    "    \"LightGBM Ranker\", \n",
    "    \"XGBoost Ranker\", \n",
    "    \"CatBoost Ranker\"\n",
    "]\n",
    "\n",
    "# Define the days_before for x-axis, reversed order\n",
    "days_before = [entry['days_before'] for entry in data[\"Real-Time Prediction\"]]\n",
    "\n",
    "# Increase the figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot each model's data\n",
    "for model in model_names:\n",
    "    if model == \"Real-Time Prediction\":\n",
    "        feature_mae = [entry['feature_mean_absolute_error'] for entry in data[model]]\n",
    "    else:\n",
    "        feature_mae = [entry['average_mean_absolute_error'] for entry in data[model]]\n",
    "    \n",
    "    # Plot with points showing for each entry\n",
    "    plt.plot(days_before, feature_mae, marker='o', label=model)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel(\"Days Before Target / Lead Day\")\n",
    "plt.ylabel(\"Mean Absolute Error\")\n",
    "plt.title(\"Average Mean Absolute Error for All Models over 21 Days Before Target Across 52 Iterations\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which='both', axis='both')\n",
    "plt.xticks(range(21, -7, -1))  # X-axis ticks from 17 to -6 with a step of 1\n",
    "plt.yticks([i/100 for i in range(0, 17, 1)])  # Y-axis ticks from 0 to 0.08 with a step of 0.01\n",
    "\n",
    "# Reverse the x-axis direction\n",
    "plt.gca().invert_xaxis()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data from the json file\n",
    "with open('short_term_results.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the first three model names\n",
    "model_names = [\n",
    "    \"Real-Time Prediction\", \n",
    "    \"Linear Regression\", \n",
    "    \"LightGBM\"\n",
    "]\n",
    "\n",
    "# Line styles and markers for distinguishing lines\n",
    "line_styles = ['-', '--', '-.']\n",
    "markers = ['o', 's', '^']  # Different marker styles\n",
    "colors = ['b', 'g', 'r']   # Different colors for each line\n",
    "\n",
    "# Filter the days_before from lead day 17 onwards\n",
    "days_before = [entry['days_before'] for entry in data[\"Real-Time Prediction\"] if entry['days_before'] <= 17]\n",
    "\n",
    "# Increase the figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot each model's data from lead day 17 onwards with different styles\n",
    "for idx, model in enumerate(model_names):\n",
    "    if model == \"Real-Time Prediction\":\n",
    "        feature_mae = [entry['feature_mean_absolute_error'] for entry in data[model] if entry['days_before'] <= 17]\n",
    "    else:\n",
    "        feature_mae = [entry['average_mean_absolute_error'] for entry in data[model] if entry['days_before'] <= 17]\n",
    "    \n",
    "    # Plot with different line styles, markers, and colors\n",
    "    plt.plot(days_before, feature_mae, marker=markers[idx], linestyle=line_styles[idx], color=colors[idx], label=model)\n",
    "\n",
    "# Create the ensemble of minimum MAE points across the three models\n",
    "ensemble_mae = [\n",
    "    min(\n",
    "        entry['feature_mean_absolute_error'] if 'feature_mean_absolute_error' in entry else float('inf'),\n",
    "        data['Linear Regression'][i]['average_mean_absolute_error'],\n",
    "        data['LightGBM'][i]['average_mean_absolute_error']\n",
    "    ) for i, entry in enumerate(data[\"Real-Time Prediction\"]) if entry['days_before'] <= 17\n",
    "]\n",
    "\n",
    "# Plot the ensemble model with a distinct style\n",
    "plt.plot(days_before, ensemble_mae, marker='D', linestyle='--', color='purple', label=\"Piecewise LightGBM and Linear\")\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel(\"Days Before Target / Lead Day\")\n",
    "plt.ylabel(\"Mean Absolute Error\")\n",
    "plt.title(\"Average Mean Absolute Error for Selected Models and Piecewise Model from Lead Day 17 Onwards\")\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Add gridlines for every 1 day on the x-axis (17 to -6) and every 0.01 on the y-axis (0 to 0.08)\n",
    "plt.grid(True, which='both', axis='both')\n",
    "plt.xticks(range(17, -7, -1))  # X-axis ticks from 17 to -6 with a step of 1\n",
    "plt.yticks([i/100 for i in range(0, 9, 1)])  # Y-axis ticks from 0 to 0.08 with a step of 0.01\n",
    "\n",
    "# Reverse the x-axis direction\n",
    "plt.gca().invert_xaxis()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-env-harshal-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
